{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Event-Based Anomaly Detection: CPI â†’ HY OAS & Unemployment Events\n",
        "\n",
        "> **ðŸ“š Detailed Methodology**: See [`docs/cpi_hy_unemployment_methodology.md`](../docs/cpi_hy_unemployment_methodology.md)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Detect large moves in target variables around macro announcements:\n",
        "\n",
        "| Model | Trigger | Target | \n",
        "|-------|---------|--------|\n",
        "| CPI â†’ HY OAS | CPI release | High-Yield spread change |\n",
        "| Unemp â†’ 10Y | Jobs report | 10Y yield change |\n",
        "| Unemp â†’ HY | Jobs report | HY spread change |\n",
        "| Unemp â†’ VIX | Jobs report | VIX change (note: absolute points, not relative) |\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "| Step | Method | Details in Doc |\n",
        "|------|--------|----------------|\n",
        "| Dataset | Event-level, t-1 features only | Section 2-3 |\n",
        "| Split | Chronological 70/30 | Section 4 |\n",
        "| \"Large\" threshold | 85th pctl of train |Î”| | Section 5 |\n",
        "| Model | RF, class_weight={F:1, T:50} | Section 6 |\n",
        "| Deployment threshold | TimeSeriesSplit CV, median | Section 7 |\n",
        "| Ex-post diagnostics | FN=0, FNâ‰¤1, FN<1% | Section 8 |\n",
        "\n",
        "## Key Design Principles\n",
        "\n",
        "- **Cost-sensitive**: Prioritize catching all large moves (FN=0)\n",
        "- **Time-based splits**: No future data leakage\n",
        "- **Training-only thresholds**: Large move threshold AND deployment threshold from train only\n",
        "- **Two types of threshold analysis**:\n",
        "  - **CV threshold** (Section 7): What we'd deploy live\n",
        "  - **Ex-post scenarios** (Section 8): Diagnostics on test set tradeoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methodology documentation: docs/cpi_hy_unemployment_methodology.md\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from src.data.merge_panel import build_fed_panel\n",
        "from src.data.inflation_announcements_loader import load_inflation_announcements\n",
        "from src.data.fred_loader import load_series\n",
        "from src.utils.logging_utils import get_logger, setup_logging\n",
        "\n",
        "setup_logging()\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('Imports complete. See docs/cpi_hy_unemployment_methodology.md for full methodology.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: CPI â†’ HY OAS Edge\n",
        "\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) sections 2-3 for feature engineering details.\n",
        "\n",
        "Detect large HY spread moves around CPI releases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_prev_month(period: str) -> str:\n",
        "    \"\"\"Get previous month's period string (YYYY-MM format).\"\"\"\n",
        "    year, month = map(int, period.split('-'))\n",
        "    if month == 1:\n",
        "        return f\"{year - 1}-12\"\n",
        "    else:\n",
        "        return f\"{year}-{month - 1:02d}\"\n",
        "\n",
        "\n",
        "def build_cpi_hy_dataset(\n",
        "    panel: pd.DataFrame,\n",
        "    cpi_announcements: pd.DataFrame,\n",
        "    window_days: int = 0,\n",
        "    min_history_days: int = 30,\n",
        "    yield_vol_window: int = 20\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build event-level dataset for CPI â†’ HY OAS large move detection.\n",
        "    \n",
        "    Args:\n",
        "        panel: Daily FRED panel with columns including hy_oas, y_2y, y_10y, etc.\n",
        "        cpi_announcements: CPI release calendar with release_date, data_period\n",
        "        window_days: Days after announcement to measure change (0 = same day)\n",
        "        min_history_days: Minimum days of history required\n",
        "        yield_vol_window: Rolling window for yield volatility calculation\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with one row per CPI event, containing features and HY OAS change\n",
        "    \"\"\"\n",
        "    # Check required columns in panel\n",
        "    required_cols = ['hy_oas', 'y_2y', 'y_10y', 'slope_10y_2y', 'fed_funds', 'unemployment', 'cpi']\n",
        "    missing_cols = [c for c in required_cols if c not in panel.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Need {missing_cols} in the panel for CPIâ†’HY model.\")\n",
        "    \n",
        "    # Ensure date columns are datetime\n",
        "    panel = panel.copy()\n",
        "    panel['date'] = pd.to_datetime(panel['date'])\n",
        "    panel = panel.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    cpi_announcements = cpi_announcements.copy()\n",
        "    cpi_announcements['release_date'] = pd.to_datetime(cpi_announcements['release_date'])\n",
        "    \n",
        "    # Load raw CPI for MoM calculation\n",
        "    try:\n",
        "        cpi_raw = load_series(\"CPIAUCSL\")\n",
        "        cpi_raw = cpi_raw.reset_index()\n",
        "        cpi_raw.columns = ['date', 'cpi_value']\n",
        "        cpi_raw['date'] = pd.to_datetime(cpi_raw['date'])\n",
        "        cpi_raw['year_month'] = cpi_raw['date'].dt.to_period('M')\n",
        "        cpi_monthly = cpi_raw.groupby('year_month')['cpi_value'].last().reset_index()\n",
        "        cpi_monthly['data_period'] = cpi_monthly['year_month'].astype(str)\n",
        "        cpi_dict = dict(zip(cpi_monthly['data_period'], cpi_monthly['cpi_value']))\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not load raw CPI: {e}. Using panel values.\")\n",
        "        cpi_dict = {}\n",
        "    \n",
        "    events = []\n",
        "    \n",
        "    for idx, ann in cpi_announcements.iterrows():\n",
        "        ann_date = ann['release_date']\n",
        "        data_period = ann['data_period']\n",
        "        \n",
        "        # Find panel row for announcement date\n",
        "        ann_day = panel[panel['date'] == ann_date]\n",
        "        if ann_day.empty:\n",
        "            # Try nearby dates (weekend/holiday adjustment)\n",
        "            for offset in [1, -1, 2, -2, 3, -3]:\n",
        "                candidate = ann_date + pd.Timedelta(days=offset)\n",
        "                ann_day = panel[panel['date'] == candidate]\n",
        "                if not ann_day.empty:\n",
        "                    ann_date = candidate\n",
        "                    break\n",
        "        \n",
        "        if ann_day.empty:\n",
        "            continue\n",
        "        \n",
        "        ann_row = ann_day.iloc[0]\n",
        "        \n",
        "        # Get previous trading day\n",
        "        prev_days = panel[panel['date'] < ann_date]\n",
        "        if len(prev_days) < min_history_days:\n",
        "            continue\n",
        "        prev_row = prev_days.iloc[-1]\n",
        "        \n",
        "        # Get HY OAS before and after\n",
        "        hy_before = prev_row.get('hy_oas')\n",
        "        \n",
        "        if window_days == 0:\n",
        "            hy_after = ann_row.get('hy_oas')\n",
        "        else:\n",
        "            future_date = ann_date + pd.Timedelta(days=window_days)\n",
        "            future_rows = panel[panel['date'] >= future_date]\n",
        "            if future_rows.empty:\n",
        "                continue\n",
        "            hy_after = future_rows.iloc[0].get('hy_oas')\n",
        "        \n",
        "        if pd.isna(hy_before) or pd.isna(hy_after):\n",
        "            continue\n",
        "        \n",
        "        hy_change = hy_after - hy_before\n",
        "        \n",
        "        # Compute CPI surprise (MoM)\n",
        "        current_cpi = cpi_dict.get(data_period, ann_row.get('cpi'))\n",
        "        prev_month = _get_prev_month(data_period)\n",
        "        prev_cpi = cpi_dict.get(prev_month)\n",
        "        \n",
        "        if prev_cpi and current_cpi and prev_cpi > 0:\n",
        "            cpi_shock_mom = ((current_cpi - prev_cpi) / prev_cpi) * 100\n",
        "        else:\n",
        "            cpi_shock_mom = np.nan\n",
        "        \n",
        "        # Calculate yield volatility (trailing window)\n",
        "        recent = prev_days.tail(yield_vol_window)\n",
        "        if len(recent) > 5:\n",
        "            y10_vals = recent['y_10y'].dropna().values\n",
        "            if len(y10_vals) > 5:\n",
        "                yield_volatility = np.std(np.diff(y10_vals))\n",
        "            else:\n",
        "                yield_volatility = np.nan\n",
        "        else:\n",
        "            yield_volatility = np.nan\n",
        "        \n",
        "        event = {\n",
        "            'date': ann_date,\n",
        "            'data_period': data_period,\n",
        "            # CPI features\n",
        "            'cpi_shock_mom': cpi_shock_mom,\n",
        "            'cpi_shock_abs': abs(cpi_shock_mom) if pd.notna(cpi_shock_mom) else np.nan,\n",
        "            # Background features (t-1)\n",
        "            'yield_volatility': yield_volatility,\n",
        "            'slope_10y_2y': prev_row.get('slope_10y_2y'),\n",
        "            'fed_funds': prev_row.get('fed_funds'),\n",
        "            'unemployment': prev_row.get('unemployment'),\n",
        "            'hy_oas_before': hy_before,\n",
        "            'y_10y_before': prev_row.get('y_10y'),\n",
        "            # Stress indicators\n",
        "            'stlfsi': prev_row.get('stlfsi', np.nan),\n",
        "            'vix': prev_row.get('vix', np.nan),\n",
        "            # Target\n",
        "            'hy_oas_change': hy_change,\n",
        "            'hy_oas_change_abs': abs(hy_change),\n",
        "        }\n",
        "        events.append(event)\n",
        "    \n",
        "    df = pd.DataFrame(events)\n",
        "    if df.empty:\n",
        "        logger.warning(\"No events created. Check data availability.\")\n",
        "        return df\n",
        "    \n",
        "    df = df.dropna(subset=['hy_oas_change'])\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    logger.info(f\"Built CPIâ†’HY dataset: {len(df)} events from {df['date'].min().date()} to {df['date'].max().date()}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading panel data...\")\n",
        "panel = build_fed_panel()\n",
        "print(f\"Panel shape: {panel.shape}\")\n",
        "print(f\"Panel columns: {list(panel.columns)}\")\n",
        "\n",
        "print(\"\\nLoading CPI announcements...\")\n",
        "cpi_announcements = load_inflation_announcements()\n",
        "print(f\"CPI announcements: {len(cpi_announcements)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CPI â†’ HY dataset\n",
        "cpi_hy_df = build_cpi_hy_dataset(panel, cpi_announcements, window_days=0)\n",
        "print(f\"\\nCPIâ†’HY Events: {len(cpi_hy_df)}\")\n",
        "print(f\"Date range: {cpi_hy_df['date'].min().date()} to {cpi_hy_df['date'].max().date()}\")\n",
        "cpi_hy_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore HY OAS changes distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.hist(cpi_hy_df['hy_oas_change'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "ax1.set_xlabel('HY OAS Change (pp)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of HY OAS Changes Around CPI Releases')\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(cpi_hy_df['cpi_shock_mom'], cpi_hy_df['hy_oas_change'], alpha=0.5)\n",
        "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax2.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('CPI Shock MoM (%)')\n",
        "ax2.set_ylabel('HY OAS Change (pp)')\n",
        "ax2.set_title('CPI Shock vs HY OAS Change')\n",
        "\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(cpi_hy_df['hy_oas_before'], cpi_hy_df['hy_oas_change'], alpha=0.5)\n",
        "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax3.set_xlabel('HY OAS Before (pp)')\n",
        "ax3.set_ylabel('HY OAS Change (pp)')\n",
        "ax3.set_title('HY OAS Level vs Change')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHY OAS Change Statistics:\")\n",
        "print(cpi_hy_df['hy_oas_change'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Test Split and Threshold\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) sections 4-5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_large_move_threshold_from_train(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    percentile: float = 90.0\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute 'large move' threshold from training data only.\n",
        "    \n",
        "    Args:\n",
        "        df: Training DataFrame\n",
        "        target_col: Column name for the target change (e.g., 'hy_oas_change')\n",
        "        percentile: Percentile of |change| to use as threshold (e.g., 90 = top 10%)\n",
        "    \n",
        "    Returns:\n",
        "        Threshold value in same units as target\n",
        "    \"\"\"\n",
        "    abs_changes = df[target_col].abs().dropna()\n",
        "    threshold = np.percentile(abs_changes, percentile)\n",
        "    n_large = (abs_changes >= threshold).sum()\n",
        "    \n",
        "    logger.info(f\"Large move threshold ({percentile}th percentile): {threshold:.4f}\")\n",
        "    logger.info(f\"Large moves in training: {n_large} ({n_large/len(abs_changes)*100:.1f}%)\")\n",
        "    \n",
        "    return threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chronological train/test split (70/30)\n",
        "n_total = len(cpi_hy_df)\n",
        "n_test = int(n_total * 0.30)\n",
        "n_train = n_total - n_test\n",
        "\n",
        "train_df = cpi_hy_df.iloc[:n_train].copy()\n",
        "test_df = cpi_hy_df.iloc[n_train:].copy()\n",
        "\n",
        "print(\"DATA SPLITS (chronological)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train: {len(train_df)} events ({train_df['date'].min().date()} to {train_df['date'].max().date()})\")\n",
        "print(f\"Test:  {len(test_df)} events ({test_df['date'].min().date()} to {test_df['date'].max().date()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute large move threshold from TRAINING DATA ONLY\n",
        "# Using 85th percentile of |HY OAS change| as \"large\" (more events for better CV)\n",
        "LARGE_HY_THRESHOLD = compute_large_move_threshold_from_train(\n",
        "    train_df, 'hy_oas_change', percentile=85.0\n",
        ")\n",
        "\n",
        "print(f\"\\nLARGE HY OAS THRESHOLD: {LARGE_HY_THRESHOLD:.4f} percentage points\")\n",
        "print(f\"(Movements above {LARGE_HY_THRESHOLD*100:.1f} bps will be labeled as 'large')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create binary labels\n",
        "train_df['is_large_hy_move'] = train_df['hy_oas_change'].abs() >= LARGE_HY_THRESHOLD\n",
        "test_df['is_large_hy_move'] = test_df['hy_oas_change'].abs() >= LARGE_HY_THRESHOLD\n",
        "\n",
        "print(\"LABEL DISTRIBUTION\")\n",
        "print(\"-\"*40)\n",
        "print(f\"Train large moves: {train_df['is_large_hy_move'].sum()} ({train_df['is_large_hy_move'].mean()*100:.1f}%)\")\n",
        "print(f\"Test large moves:  {test_df['is_large_hy_move'].sum()} ({test_df['is_large_hy_move'].mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) section 6 for cost-sensitive RF details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cost_sensitive_model(\n",
        "    train_df: pd.DataFrame,\n",
        "    feature_cols: list,\n",
        "    target_col: str,\n",
        "    class_weight_positive: int = 50,\n",
        "    n_cv_splits: int = 5,\n",
        "    model_type: str = 'random_forest'\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Train a cost-sensitive classifier using time-series cross-validation.\n",
        "    \n",
        "    Prioritizes FN=0 (never miss a large move) over minimizing FP.\n",
        "    \n",
        "    Args:\n",
        "        train_df: Training DataFrame\n",
        "        feature_cols: List of feature column names\n",
        "        target_col: Binary target column name\n",
        "        class_weight_positive: Weight for positive class (large moves)\n",
        "        n_cv_splits: Number of time series CV splits\n",
        "        model_type: 'random_forest' or 'gradient_boosting'\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with model, scaler, medians, threshold, and CV metrics\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    X = train_df[feature_cols].copy()\n",
        "    y = train_df[target_col].values\n",
        "    \n",
        "    # Compute medians for imputation (from training data only)\n",
        "    medians = {col: X[col].median() for col in feature_cols}\n",
        "    X = X.fillna(medians)\n",
        "    \n",
        "    # Fit scaler on training data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Time series CV for threshold selection\n",
        "    tscv = TimeSeriesSplit(n_splits=n_cv_splits)\n",
        "    cv_thresholds = []\n",
        "    cv_fp_rates = []\n",
        "    \n",
        "    print(f\"TIME SERIES CROSS-VALIDATION ({n_cv_splits} folds)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled)):\n",
        "        X_tr = X_scaled[train_idx]\n",
        "        X_val = X_scaled[val_idx]\n",
        "        y_tr = y[train_idx]\n",
        "        y_val = y[val_idx]\n",
        "        \n",
        "        # Skip if insufficient positives in training or validation\n",
        "        if y_tr.sum() < 2 or y_val.sum() == 0:\n",
        "            print(f\"  Fold {fold+1}: skipped (insufficient positives)\")\n",
        "            continue\n",
        "        \n",
        "        # Train model\n",
        "        if model_type == 'random_forest':\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                class_weight={False: 1, True: class_weight_positive},\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            model = GradientBoostingClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=3,\n",
        "                random_state=42\n",
        "            )\n",
        "        \n",
        "        if model_type == 'gradient_boosting':\n",
        "            sample_weight = np.where(y_tr, class_weight_positive, 1)\n",
        "            model.fit(X_tr, y_tr, sample_weight=sample_weight)\n",
        "        else:\n",
        "            model.fit(X_tr, y_tr)\n",
        "        \n",
        "        # Check model has both classes\n",
        "        if len(model.classes_) < 2:\n",
        "            print(f\"  Fold {fold+1}: skipped (single class in model)\")\n",
        "            continue\n",
        "        \n",
        "        # Get probabilities for positive class\n",
        "        val_probs = model.predict_proba(X_val)[:, 1]\n",
        "        \n",
        "        # Use 5th percentile of positive class probs (more robust than min)\n",
        "        positive_probs = val_probs[y_val]\n",
        "        thresh = max(0.05, np.percentile(positive_probs, 5) - 0.01)\n",
        "        \n",
        "        # Compute FP rate and FN count at this threshold\n",
        "        val_pred = val_probs >= thresh\n",
        "        fn = (y_val.astype(bool) & ~val_pred).sum()\n",
        "        fp_rate = (~y_val.astype(bool) & val_pred).sum() / max(1, (~y_val.astype(bool)).sum())\n",
        "        \n",
        "        cv_thresholds.append(thresh)\n",
        "        cv_fp_rates.append(fp_rate)\n",
        "        \n",
        "        print(f\"  Fold {fold+1}: threshold={thresh:.4f}, FN={fn}, FP rate={fp_rate:.1%}\")\n",
        "    \n",
        "    # Use MEDIAN threshold (more stable than min)\n",
        "    if cv_thresholds:\n",
        "        robust_threshold = np.median(cv_thresholds)\n",
        "    else:\n",
        "        robust_threshold = 0.10  # Default fallback\n",
        "    \n",
        "    print(f\"\\nRobust threshold (median across folds): {robust_threshold:.4f}\")\n",
        "    \n",
        "    # Train final model on ALL training data\n",
        "    print(\"\\nFINAL MODEL TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if model_type == 'random_forest':\n",
        "        final_model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=5,\n",
        "            class_weight={False: 1, True: class_weight_positive},\n",
        "            random_state=42\n",
        "        )\n",
        "        final_model.fit(X_scaled, y)\n",
        "    else:\n",
        "        final_model = GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=3,\n",
        "            random_state=42\n",
        "        )\n",
        "        sample_weight = np.where(y, class_weight_positive, 1)\n",
        "        final_model.fit(X_scaled, y, sample_weight=sample_weight)\n",
        "    \n",
        "    print(f\"Model trained on {len(y)} events\")\n",
        "    print(f\"Threshold from CV: {robust_threshold:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'scaler': scaler,\n",
        "        'medians': medians,\n",
        "        'threshold': robust_threshold,\n",
        "        'cv_thresholds': cv_thresholds,\n",
        "        'cv_fp_rates': cv_fp_rates,\n",
        "        'feature_cols': feature_cols,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features for CPI â†’ HY model\n",
        "CPI_HY_FEATURES = [\n",
        "    'cpi_shock_mom',      # CPI surprise\n",
        "    'cpi_shock_abs',      # |CPI surprise|\n",
        "    'yield_volatility',   # Recent yield volatility\n",
        "    'slope_10y_2y',       # Yield curve slope\n",
        "    'fed_funds',          # Fed funds rate\n",
        "    'unemployment',       # Unemployment rate\n",
        "    'hy_oas_before',      # HY OAS level before\n",
        "]\n",
        "\n",
        "# Train model\n",
        "cpi_hy_result = train_cost_sensitive_model(\n",
        "    train_df=train_df,\n",
        "    feature_cols=CPI_HY_FEATURES,\n",
        "    target_col='is_large_hy_move',\n",
        "    class_weight_positive=50,\n",
        "    n_cv_splits=5,\n",
        "    model_type='random_forest'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Evaluation (Ex-Post Diagnostics)\n",
        "\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) sections 8-9.\n",
        "\n",
        "**Note**: The FN=0/FNâ‰¤1/FN<1% scenarios below are **ex-post diagnostics** computed on the test set. They show \"what threshold *would have been* needed\" for each FN constraint on this specific test period. These are NOT deployment thresholdsâ€”they help us understand the tradeoff frontier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_triple_thresholds(\n",
        "    test_df: pd.DataFrame,\n",
        "    model_result: dict,\n",
        "    target_col: str,\n",
        "    target_name: str = \"target\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate model on test set with THREE threshold scenarios:\n",
        "    - FN=0: Catch ALL large moves\n",
        "    - FNâ‰¤1: Allow at most 1 missed\n",
        "    - FN<1%: Miss less than 1% of positives\n",
        "    \n",
        "    Args:\n",
        "        test_df: Test DataFrame\n",
        "        model_result: Dictionary from train_cost_sensitive_model\n",
        "        target_col: Binary target column name\n",
        "        target_name: Human-readable name for the target\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics for all scenarios\n",
        "    \"\"\"\n",
        "    model = model_result['model']\n",
        "    scaler = model_result['scaler']\n",
        "    medians = model_result['medians']\n",
        "    feature_cols = model_result['feature_cols']\n",
        "    \n",
        "    # Prepare test features (using TRAINING medians and scaler)\n",
        "    X_test = test_df[feature_cols].copy()\n",
        "    X_test = X_test.fillna(medians)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_test = test_df[target_col].values\n",
        "    \n",
        "    # Get predictions\n",
        "    test_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    \n",
        "    n_positive = y_test.sum()\n",
        "    n_negative = (~y_test).sum()\n",
        "    \n",
        "    # Get sorted positive probabilities\n",
        "    if n_positive > 0:\n",
        "        sorted_pos_probs = np.sort(test_probs[y_test])\n",
        "    else:\n",
        "        sorted_pos_probs = np.array([0.5])\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Scenario 1: FN=0 (catch ALL large moves)\n",
        "    thresh_fn0 = sorted_pos_probs[0] - 0.001 if n_positive > 0 else 0.5\n",
        "    pred = test_probs >= thresh_fn0\n",
        "    results['fn0'] = {\n",
        "        'threshold': thresh_fn0,\n",
        "        'TP': (y_test & pred).sum(),\n",
        "        'FP': (~y_test & pred).sum(),\n",
        "        'FN': (y_test & ~pred).sum(),\n",
        "        'TN': (~y_test & ~pred).sum(),\n",
        "    }\n",
        "    \n",
        "    # Scenario 2: FNâ‰¤1 (allow at most 1 missed)\n",
        "    if n_positive > 1:\n",
        "        thresh_fn1 = sorted_pos_probs[1] - 0.001  # 2nd lowest positive prob\n",
        "    else:\n",
        "        thresh_fn1 = thresh_fn0\n",
        "    thresh_fn1 = max(thresh_fn1, 0.05)\n",
        "    pred = test_probs >= thresh_fn1\n",
        "    results['fn_le_1'] = {\n",
        "        'threshold': thresh_fn1,\n",
        "        'TP': (y_test & pred).sum(),\n",
        "        'FP': (~y_test & pred).sum(),\n",
        "        'FN': (y_test & ~pred).sum(),\n",
        "        'TN': (~y_test & ~pred).sum(),\n",
        "    }\n",
        "    \n",
        "    # Scenario 3: FN<1% (miss less than 1% of positives)\n",
        "    max_fn = max(1, int(np.ceil(0.01 * n_positive)))\n",
        "    if n_positive > max_fn:\n",
        "        thresh_fn1pct = sorted_pos_probs[max_fn] - 0.001\n",
        "    else:\n",
        "        thresh_fn1pct = thresh_fn0\n",
        "    thresh_fn1pct = max(thresh_fn1pct, 0.05)\n",
        "    pred = test_probs >= thresh_fn1pct\n",
        "    results['fn_lt_1pct'] = {\n",
        "        'threshold': thresh_fn1pct,\n",
        "        'TP': (y_test & pred).sum(),\n",
        "        'FP': (~y_test & pred).sum(),\n",
        "        'FN': (y_test & ~pred).sum(),\n",
        "        'TN': (~y_test & ~pred).sum(),\n",
        "    }\n",
        "    \n",
        "    # Calculate rates for each scenario\n",
        "    for key in ['fn0', 'fn_le_1', 'fn_lt_1pct']:\n",
        "        r = results[key]\n",
        "        r['fn_rate'] = r['FN'] / max(1, n_positive)\n",
        "        r['fp_rate'] = r['FP'] / max(1, n_negative)\n",
        "        r['tn_rate'] = r['TN'] / max(1, n_negative)  # True Negative Rate (Specificity)\n",
        "        r['precision'] = r['TP'] / max(1, r['TP'] + r['FP'])\n",
        "        r['recall'] = r['TP'] / max(1, r['TP'] + r['FN'])\n",
        "    \n",
        "    results['n_positive'] = n_positive\n",
        "    results['n_negative'] = n_negative\n",
        "    results['test_probs'] = test_probs\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(\"=\"*80)\n",
        "    print(f\"TEST SET EVALUATION: {target_name}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTest pool: {n_positive} large moves (+), {n_negative} normal events (-)\")\n",
        "    \n",
        "    print(f\"\\n{'Scenario':<12} â”‚ {'Thresh':>7} â”‚ {'TP':>4} {'FP':>4} {'FN':>4} {'TN':>4} â”‚ {'FN%':>6} {'FP%':>6} {'TN%':>6} â”‚ {'Prec':>6}\")\n",
        "    print(\"â”€\"*80)\n",
        "    \n",
        "    for scenario, label in [('fn0', 'FN=0'), ('fn_le_1', 'FNâ‰¤1'), ('fn_lt_1pct', 'FN<1%')]:\n",
        "        r = results[scenario]\n",
        "        print(f\"{label:<12} â”‚ {r['threshold']:>7.4f} â”‚ {r['TP']:>4} {r['FP']:>4} {r['FN']:>4} {r['TN']:>4} â”‚ \"\n",
        "              f\"{r['fn_rate']*100:>5.1f}% {r['fp_rate']*100:>5.1f}% {r['tn_rate']*100:>5.1f}% â”‚ {r['precision']:>5.1%}\")\n",
        "    \n",
        "    # Show impact of relaxing constraint\n",
        "    fn0_r = results['fn0']\n",
        "    fn1_r = results['fn_le_1']\n",
        "    tn_gain = fn1_r['TN'] - fn0_r['TN']\n",
        "    fp_reduction = fn0_r['FP'] - fn1_r['FP']\n",
        "    fn_cost = fn1_r['FN'] - fn0_r['FN']\n",
        "    \n",
        "    print(f\"\\nðŸ“Š FN=0 â†’ FNâ‰¤1 tradeoff:\")\n",
        "    if fp_reduction > 0:\n",
        "        print(f\"   Saves {fp_reduction} false positives, gains {tn_gain} true negatives\")\n",
        "        print(f\"   Cost: {fn_cost} additional missed large move(s)\")\n",
        "    else:\n",
        "        print(f\"   No improvement (same threshold)\")\n",
        "    \n",
        "    # Find structurally unpredictable events\n",
        "    unpredictable_mask = (test_probs < 0.05) & y_test\n",
        "    n_unpredictable = unpredictable_mask.sum()\n",
        "    results['n_unpredictable'] = n_unpredictable\n",
        "    \n",
        "    if n_unpredictable > 0:\n",
        "        print(f\"\\nâš  STRUCTURALLY UNPREDICTABLE (p<5% but large move): {n_unpredictable}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Backwards compatibility alias\n",
        "def evaluate_model_on_test(test_df, model_result, target_col, target_name=\"target\", adjust_for_zero_fn=True):\n",
        "    \"\"\"Wrapper for backwards compatibility - uses new triple threshold evaluation.\"\"\"\n",
        "    return evaluate_model_triple_thresholds(test_df, model_result, target_col, target_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "cpi_hy_eval = evaluate_model_on_test(\n",
        "    test_df=test_df,\n",
        "    model_result=cpi_hy_result,\n",
        "    target_col='is_large_hy_move',\n",
        "    target_name='CPI â†’ HY OAS Large Move'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results with all three scenarios\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Probability distribution\n",
        "ax1 = axes[0, 0]\n",
        "y_test = test_df['is_large_hy_move'].values\n",
        "test_probs = cpi_hy_eval['test_probs']\n",
        "ax1.hist(test_probs[~y_test], bins=30, alpha=0.6, label='Normal', color='blue')\n",
        "ax1.hist(test_probs[y_test], bins=15, alpha=0.6, label='Large Move', color='red')\n",
        "\n",
        "# Show all three thresholds\n",
        "ax1.axvline(x=cpi_hy_eval['fn0']['threshold'], color='green', linestyle='--', linewidth=2,\n",
        "            label=f\"FN=0 thresh={cpi_hy_eval['fn0']['threshold']:.3f}\")\n",
        "ax1.axvline(x=cpi_hy_eval['fn_le_1']['threshold'], color='orange', linestyle=':', linewidth=2,\n",
        "            label=f\"FNâ‰¤1 thresh={cpi_hy_eval['fn_le_1']['threshold']:.3f}\")\n",
        "ax1.set_xlabel('Predicted Probability')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('CPIâ†’HY: Test Set Probability Distribution')\n",
        "ax1.legend(fontsize=8)\n",
        "\n",
        "# Confusion matrices for each scenario\n",
        "for idx, (scenario, title) in enumerate([('fn0', 'FN=0'), ('fn_le_1', 'FNâ‰¤1'), ('fn_lt_1pct', 'FN<1%')]):\n",
        "    ax = axes[(idx+1)//2, (idx+1)%2]\n",
        "    r = cpi_hy_eval[scenario]\n",
        "    cm = np.array([[r['TN'], r['FP']],\n",
        "                   [r['FN'], r['TP']]])\n",
        "    im = ax.imshow(cm, cmap='Blues')\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels(['Pred Normal', 'Pred Large'])\n",
        "    ax.set_yticklabels(['Actual Normal', 'Actual Large'])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            ax.text(j, i, cm[i, j], ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'CPIâ†’HY: {title} (FP={r[\"FP\"]}, TN={r[\"TN\"]})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print key insight\n",
        "print(\"\\nðŸ“Š CPIâ†’HY KEY INSIGHT:\")\n",
        "fn0 = cpi_hy_eval['fn0']\n",
        "fn1 = cpi_hy_eval['fn_le_1']\n",
        "print(f\"   At FN=0: FP={fn0['FP']}, TN={fn0['TN']} (TN%={fn0['tn_rate']*100:.1f}%)\")\n",
        "print(f\"   At FNâ‰¤1: FP={fn1['FP']}, TN={fn1['TN']} (TN%={fn1['tn_rate']*100:.1f}%)\")\n",
        "print(f\"   â†’ Relaxing to FNâ‰¤1 saves {fn0['FP']-fn1['FP']} false positives\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Unemployment Event Family\n",
        "\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) for full details.\n",
        "\n",
        "Same approach as CPI, applied to unemployment releases â†’ 10Y, HY OAS, VIX targets.\n",
        "\n",
        "**Note**: Unemployment calendar is approximate (first Friday of month). TODO: Replace with official BLS dates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_or_create_unemployment_calendar(\n",
        "    panel: pd.DataFrame,\n",
        "    calendar_path: str = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load unemployment release calendar or create an approximation.\n",
        "    \n",
        "    The unemployment report (Jobs Report) is released on the first Friday of each month.\n",
        "    \n",
        "    Args:\n",
        "        panel: Daily FRED panel with 'unemployment' column\n",
        "        calendar_path: Path to unemployment calendar CSV (optional)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with release_date, data_period, reported_unemployment, \n",
        "        previous_unemployment, unemployment_surprise\n",
        "    \"\"\"\n",
        "    # Try to load from file first\n",
        "    if calendar_path:\n",
        "        try:\n",
        "            cal = pd.read_csv(calendar_path, parse_dates=['release_date'])\n",
        "            logger.info(f\"Loaded unemployment calendar from {calendar_path}\")\n",
        "            return cal\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load unemployment calendar: {e}. Creating approximation.\")\n",
        "    \n",
        "    # Create approximate calendar based on first Friday of each month\n",
        "    # TODO: Replace with actual BLS release times from official source\n",
        "    logger.info(\"Creating approximate unemployment release calendar (first Friday of each month)\")\n",
        "    \n",
        "    panel = panel.copy()\n",
        "    panel['date'] = pd.to_datetime(panel['date'])\n",
        "    panel = panel.sort_values('date')\n",
        "    \n",
        "    # Get unique months with unemployment data\n",
        "    panel_with_unemp = panel.dropna(subset=['unemployment'])\n",
        "    \n",
        "    # Find first Friday of each month\n",
        "    releases = []\n",
        "    \n",
        "    for year in panel_with_unemp['date'].dt.year.unique():\n",
        "        for month in range(1, 13):\n",
        "            # Find the first Friday of this month\n",
        "            first_day = pd.Timestamp(year=year, month=month, day=1)\n",
        "            # Days until Friday (Friday = 4)\n",
        "            days_until_friday = (4 - first_day.dayofweek) % 7\n",
        "            first_friday = first_day + pd.Timedelta(days=days_until_friday)\n",
        "            \n",
        "            # Get unemployment values\n",
        "            # The release on first Friday of month M reports data for month M-1\n",
        "            if month == 1:\n",
        "                data_period = f\"{year-1}-12\"\n",
        "            else:\n",
        "                data_period = f\"{year}-{month-1:02d}\"\n",
        "            \n",
        "            # Find nearest panel date\n",
        "            panel_dates = panel_with_unemp[panel_with_unemp['date'] <= first_friday]\n",
        "            if panel_dates.empty:\n",
        "                continue\n",
        "            \n",
        "            # Get unemployment on release date\n",
        "            release_row = panel_with_unemp[panel_with_unemp['date'] == first_friday]\n",
        "            if release_row.empty:\n",
        "                # Try nearby dates\n",
        "                for offset in [1, -1, 2, -2, 3]:\n",
        "                    candidate = first_friday + pd.Timedelta(days=offset)\n",
        "                    release_row = panel_with_unemp[panel_with_unemp['date'] == candidate]\n",
        "                    if not release_row.empty:\n",
        "                        first_friday = candidate\n",
        "                        break\n",
        "            \n",
        "            if release_row.empty:\n",
        "                continue\n",
        "            \n",
        "            reported = release_row.iloc[0]['unemployment']\n",
        "            \n",
        "            # Get previous month's unemployment\n",
        "            prev_rows = panel_with_unemp[panel_with_unemp['date'] < first_friday - pd.Timedelta(days=20)]\n",
        "            if prev_rows.empty:\n",
        "                previous = np.nan\n",
        "            else:\n",
        "                previous = prev_rows.iloc[-1]['unemployment']\n",
        "            \n",
        "            # Compute surprise (simple change)\n",
        "            surprise = reported - previous if pd.notna(previous) else np.nan\n",
        "            \n",
        "            releases.append({\n",
        "                'release_date': first_friday,\n",
        "                'data_period': data_period,\n",
        "                'reported_unemployment': reported,\n",
        "                'previous_unemployment': previous,\n",
        "                'unemployment_surprise': surprise,\n",
        "            })\n",
        "    \n",
        "    cal = pd.DataFrame(releases)\n",
        "    cal = cal.dropna(subset=['reported_unemployment', 'previous_unemployment'])\n",
        "    cal = cal.sort_values('release_date').reset_index(drop=True)\n",
        "    \n",
        "    logger.info(f\"Created unemployment calendar: {len(cal)} releases\")\n",
        "    \n",
        "    return cal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create/load unemployment calendar\n",
        "unemp_calendar = load_or_create_unemployment_calendar(panel)\n",
        "print(f\"Unemployment releases: {len(unemp_calendar)}\")\n",
        "print(f\"Date range: {unemp_calendar['release_date'].min().date()} to {unemp_calendar['release_date'].max().date()}\")\n",
        "unemp_calendar.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Builder\n",
        "Reusable function for any target (10Y, HY OAS, VIX).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_unemployment_event_dataset(\n",
        "    panel: pd.DataFrame,\n",
        "    unemp_calendar: pd.DataFrame,\n",
        "    target_series: str = 'y_10y',\n",
        "    window_days: int = 0,\n",
        "    min_history_days: int = 30,\n",
        "    yield_vol_window: int = 20\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build event-level dataset for unemployment â†’ target large move detection.\n",
        "    \n",
        "    Args:\n",
        "        panel: Daily FRED panel\n",
        "        unemp_calendar: Unemployment release calendar\n",
        "        target_series: Target series to detect moves in ('y_10y', 'hy_oas', 'vix')\n",
        "        window_days: Days after release to measure change (0 = same day)\n",
        "        min_history_days: Minimum days of history required\n",
        "        yield_vol_window: Rolling window for volatility calculation\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with one row per unemployment event\n",
        "    \"\"\"\n",
        "    # Map target_series to panel column\n",
        "    target_col_map = {\n",
        "        'y_10y': 'y_10y', 'DGS10': 'y_10y',\n",
        "        'hy_oas': 'hy_oas', 'BAMLH0A0HYM2': 'hy_oas',\n",
        "        'vix': 'vix', 'VIXCLS': 'vix',\n",
        "    }\n",
        "    target_col = target_col_map.get(target_series, target_series)\n",
        "    \n",
        "    if target_col not in panel.columns:\n",
        "        raise ValueError(f\"Need {target_col} in panel for unemploymentâ†’{target_series} model.\")\n",
        "    \n",
        "    panel = panel.copy()\n",
        "    panel['date'] = pd.to_datetime(panel['date'])\n",
        "    panel = panel.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    unemp_calendar = unemp_calendar.copy()\n",
        "    unemp_calendar['release_date'] = pd.to_datetime(unemp_calendar['release_date'])\n",
        "    \n",
        "    events = []\n",
        "    \n",
        "    for idx, rel in unemp_calendar.iterrows():\n",
        "        rel_date = rel['release_date']\n",
        "        data_period = rel['data_period']\n",
        "        \n",
        "        # Find panel row for release date\n",
        "        rel_day = panel[panel['date'] == rel_date]\n",
        "        if rel_day.empty:\n",
        "            for offset in [1, -1, 2, -2, 3, -3]:\n",
        "                candidate = rel_date + pd.Timedelta(days=offset)\n",
        "                rel_day = panel[panel['date'] == candidate]\n",
        "                if not rel_day.empty:\n",
        "                    rel_date = candidate\n",
        "                    break\n",
        "        \n",
        "        if rel_day.empty:\n",
        "            continue\n",
        "        \n",
        "        rel_row = rel_day.iloc[0]\n",
        "        \n",
        "        # Get previous trading day\n",
        "        prev_days = panel[panel['date'] < rel_date]\n",
        "        if len(prev_days) < min_history_days:\n",
        "            continue\n",
        "        prev_row = prev_days.iloc[-1]\n",
        "        \n",
        "        # Get target before and after\n",
        "        target_before = prev_row.get(target_col)\n",
        "        \n",
        "        if window_days == 0:\n",
        "            target_after = rel_row.get(target_col)\n",
        "        else:\n",
        "            future_date = rel_date + pd.Timedelta(days=window_days)\n",
        "            future_rows = panel[panel['date'] >= future_date]\n",
        "            if future_rows.empty:\n",
        "                continue\n",
        "            target_after = future_rows.iloc[0].get(target_col)\n",
        "        \n",
        "        if pd.isna(target_before) or pd.isna(target_after):\n",
        "            continue\n",
        "        \n",
        "        target_change = target_after - target_before\n",
        "        \n",
        "        # Calculate volatility (trailing window)\n",
        "        recent = prev_days.tail(yield_vol_window)\n",
        "        if len(recent) > 5:\n",
        "            vol_vals = recent[target_col].dropna().values\n",
        "            volatility = np.std(np.diff(vol_vals)) if len(vol_vals) > 5 else np.nan\n",
        "        else:\n",
        "            volatility = np.nan\n",
        "        \n",
        "        event = {\n",
        "            'date': rel_date,\n",
        "            'data_period': data_period,\n",
        "            'unemployment_surprise': rel['unemployment_surprise'],\n",
        "            'unemployment_surprise_abs': abs(rel['unemployment_surprise']) if pd.notna(rel['unemployment_surprise']) else np.nan,\n",
        "            'reported_unemployment': rel['reported_unemployment'],\n",
        "            'previous_unemployment': rel['previous_unemployment'],\n",
        "            f'{target_col}_volatility': volatility,\n",
        "            'slope_10y_2y': prev_row.get('slope_10y_2y'),\n",
        "            'fed_funds': prev_row.get('fed_funds'),\n",
        "            f'{target_col}_before': target_before,\n",
        "            'hy_oas_before': prev_row.get('hy_oas'),\n",
        "            'stlfsi': prev_row.get('stlfsi', np.nan),\n",
        "            'vix_before': prev_row.get('vix', np.nan),\n",
        "            f'{target_col}_change': target_change,\n",
        "            f'{target_col}_change_abs': abs(target_change),\n",
        "        }\n",
        "        events.append(event)\n",
        "    \n",
        "    df = pd.DataFrame(events)\n",
        "    if df.empty:\n",
        "        logger.warning(f\"No unemploymentâ†’{target_col} events created.\")\n",
        "        return df\n",
        "    \n",
        "    df = df.dropna(subset=[f'{target_col}_change'])\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    logger.info(f\"Built unemploymentâ†’{target_col} dataset: {len(df)} events\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build datasets for each target\n",
        "unemp_10y_df = build_unemployment_event_dataset(panel, unemp_calendar, target_series='y_10y')\n",
        "unemp_hy_df = build_unemployment_event_dataset(panel, unemp_calendar, target_series='hy_oas')\n",
        "unemp_vix_df = build_unemployment_event_dataset(panel, unemp_calendar, target_series='vix')\n",
        "\n",
        "print(f\"\\nUnemployment â†’ 10Y Events: {len(unemp_10y_df)}\")\n",
        "print(f\"Unemployment â†’ HY OAS Events: {len(unemp_hy_df)}\")\n",
        "print(f\"Unemployment â†’ VIX Events: {len(unemp_vix_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Evaluate Models\n",
        "Same methodology as CPI â†’ HY, applied to each target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_unemployment_model_for_target(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    test_size: float = 0.30,\n",
        "    threshold_percentile: float = 90.0,\n",
        "    class_weight_positive: int = 50,\n",
        "    model_type: str = 'random_forest'\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Train and evaluate a cost-sensitive model for unemployment â†’ target large move.\n",
        "    \"\"\"\n",
        "    target_name = target_col.replace('_change', '')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"UNEMPLOYMENT â†’ {target_name.upper()} MODEL\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Chronological split\n",
        "    n_total = len(df)\n",
        "    n_test = int(n_total * test_size)\n",
        "    n_train = n_total - n_test\n",
        "    \n",
        "    unemp_train_df = df.iloc[:n_train].copy()\n",
        "    unemp_test_df = df.iloc[n_train:].copy()\n",
        "    \n",
        "    print(f\"\\nTrain: {len(unemp_train_df)} events ({unemp_train_df['date'].min().date()} to {unemp_train_df['date'].max().date()})\")\n",
        "    print(f\"Test:  {len(unemp_test_df)} events ({unemp_test_df['date'].min().date()} to {unemp_test_df['date'].max().date()})\")\n",
        "    \n",
        "    # Compute threshold from training data\n",
        "    threshold = compute_large_move_threshold_from_train(unemp_train_df, target_col, threshold_percentile)\n",
        "    \n",
        "    # Create binary labels\n",
        "    is_large_col = f'is_large_{target_name}_move'\n",
        "    unemp_train_df[is_large_col] = unemp_train_df[target_col].abs() >= threshold\n",
        "    unemp_test_df[is_large_col] = unemp_test_df[target_col].abs() >= threshold\n",
        "    \n",
        "    print(f\"\\nTrain large moves: {unemp_train_df[is_large_col].sum()} ({unemp_train_df[is_large_col].mean()*100:.1f}%)\")\n",
        "    print(f\"Test large moves:  {unemp_test_df[is_large_col].sum()} ({unemp_test_df[is_large_col].mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Define features\n",
        "    feature_cols = [\n",
        "        'unemployment_surprise',\n",
        "        'unemployment_surprise_abs',\n",
        "        f'{target_name}_volatility',\n",
        "        'slope_10y_2y',\n",
        "        'fed_funds',\n",
        "        f'{target_name}_before',\n",
        "    ]\n",
        "    \n",
        "    # Only include columns that exist\n",
        "    feature_cols = [c for c in feature_cols if c in unemp_train_df.columns]\n",
        "    \n",
        "    # Add HY OAS before if available and not the target\n",
        "    if 'hy_oas_before' in unemp_train_df.columns and target_name != 'hy_oas':\n",
        "        feature_cols.append('hy_oas_before')\n",
        "    \n",
        "    print(f\"\\nFeatures: {feature_cols}\")\n",
        "    \n",
        "    # Train model\n",
        "    model_result = train_cost_sensitive_model(\n",
        "        train_df=unemp_train_df,\n",
        "        feature_cols=feature_cols,\n",
        "        target_col=is_large_col,\n",
        "        class_weight_positive=class_weight_positive,\n",
        "        n_cv_splits=5,\n",
        "        model_type=model_type\n",
        "    )\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    eval_result = evaluate_model_on_test(\n",
        "        test_df=unemp_test_df,\n",
        "        model_result=model_result,\n",
        "        target_col=is_large_col,\n",
        "        target_name=f\"Unemployment â†’ {target_name.upper()} Large Move\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'model_result': model_result,\n",
        "        'eval_result': eval_result,\n",
        "        'train_df': unemp_train_df,\n",
        "        'test_df': unemp_test_df,\n",
        "        'large_move_threshold': threshold,\n",
        "        'target_col': target_col,\n",
        "        'is_large_col': is_large_col,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model for Unemployment â†’ 10Y\n",
        "unemp_10y_result = train_unemployment_model_for_target(\n",
        "    df=unemp_10y_df,\n",
        "    target_col='y_10y_change',\n",
        "    test_size=0.30,\n",
        "    threshold_percentile=85.0  # Using 85th pctl for more events\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model for Unemployment â†’ HY OAS\n",
        "unemp_hy_result = train_unemployment_model_for_target(\n",
        "    df=unemp_hy_df,\n",
        "    target_col='hy_oas_change',\n",
        "    test_size=0.30,\n",
        "    threshold_percentile=85.0  # Using 85th pctl for more events\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model for Unemployment â†’ VIX\n",
        "unemp_vix_result = train_unemployment_model_for_target(\n",
        "    df=unemp_vix_df,\n",
        "    target_col='vix_change',\n",
        "    test_size=0.30,\n",
        "    threshold_percentile=85.0  # Using 85th pctl for more events\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Results Summary (Ex-Post Diagnostics)\n",
        "\n",
        "> See [methodology doc](../docs/cpi_hy_unemployment_methodology.md) section 11.\n",
        "\n",
        "**Note**: Tables below show ex-post thresholds that *would have achieved* FN=0/1/etc on this test period. The CV-selected deployment threshold (Section 7) is computed separately from training data only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Summary Table with All Three Scenarios\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPREHENSIVE SUMMARY: ALL MODELS WITH THREE THRESHOLD SCENARIOS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Collect all results\n",
        "all_results = {\n",
        "    'CPIâ†’HY': {\n",
        "        'results': cpi_hy_eval,\n",
        "        'train_n': len(train_df),\n",
        "        'test_n': len(test_df),\n",
        "        'thresh': LARGE_HY_THRESHOLD\n",
        "    },\n",
        "    'Unempâ†’10Y': {\n",
        "        'results': unemp_10y_result['eval_result'],\n",
        "        'train_n': len(unemp_10y_result['train_df']),\n",
        "        'test_n': len(unemp_10y_result['test_df']),\n",
        "        'thresh': unemp_10y_result['large_move_threshold']\n",
        "    },\n",
        "    'Unempâ†’HY': {\n",
        "        'results': unemp_hy_result['eval_result'],\n",
        "        'train_n': len(unemp_hy_result['train_df']),\n",
        "        'test_n': len(unemp_hy_result['test_df']),\n",
        "        'thresh': unemp_hy_result['large_move_threshold']\n",
        "    },\n",
        "    'Unempâ†’VIX': {\n",
        "        'results': unemp_vix_result['eval_result'],\n",
        "        'train_n': len(unemp_vix_result['train_df']),\n",
        "        'test_n': len(unemp_vix_result['test_df']),\n",
        "        'thresh': unemp_vix_result['large_move_threshold']\n",
        "    },\n",
        "}\n",
        "\n",
        "# Print summary for each scenario\n",
        "for scenario, scenario_label in [('fn0', 'FN=0 (Never miss a large move)'), \n",
        "                                  ('fn_le_1', 'FNâ‰¤1 (Allow at most 1 missed)'),\n",
        "                                  ('fn_lt_1pct', 'FN<1% (Miss less than 1%)')]:\n",
        "    print(f\"\\nâ”Œ{'â”€'*96}â”\")\n",
        "    print(f\"â”‚{scenario_label:^96}â”‚\")\n",
        "    print(f\"â”œ{'â”€'*96}â”¤\")\n",
        "    print(f\"â”‚ {'Model':<12} â”‚ {'Pool':<12} â”‚ {'TP':>4} {'FP':>4} {'FN':>4} {'TN':>4} â”‚ {'FN%':>6} {'FP%':>6} {'TN%':>6} â”‚ {'Prec':>6} â”‚\")\n",
        "    print(f\"â”œ{'â”€'*96}â”¤\")\n",
        "    \n",
        "    for name, data in all_results.items():\n",
        "        res = data['results']\n",
        "        r = res[scenario]\n",
        "        n_pos = res['n_positive']\n",
        "        n_neg = res['n_negative']\n",
        "        pool_str = f\"{n_pos}+ / {n_neg}-\"\n",
        "        \n",
        "        print(f\"â”‚ {name:<12} â”‚ {pool_str:<12} â”‚ {r['TP']:>4} {r['FP']:>4} {r['FN']:>4} {r['TN']:>4} â”‚ \"\n",
        "              f\"{r['fn_rate']*100:>5.1f}% {r['fp_rate']*100:>5.1f}% {r['tn_rate']*100:>5.1f}% â”‚ {r['precision']:>5.1%} â”‚\")\n",
        "    \n",
        "    print(f\"â””{'â”€'*96}â”˜\")\n",
        "\n",
        "# Print tradeoff analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRADEOFF ANALYSIS: FN=0 â†’ FNâ‰¤1\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<12} â”‚ {'FP Saved':>10} â”‚ {'TN Gained':>10} â”‚ {'FN Cost':>10} â”‚ {'Recommendation':<20}\")\n",
        "print(\"â”€\"*70)\n",
        "\n",
        "for name, data in all_results.items():\n",
        "    res = data['results']\n",
        "    fn0 = res['fn0']\n",
        "    fn1 = res['fn_le_1']\n",
        "    \n",
        "    fp_saved = fn0['FP'] - fn1['FP']\n",
        "    tn_gained = fn1['TN'] - fn0['TN']\n",
        "    fn_cost = fn1['FN'] - fn0['FN']\n",
        "    \n",
        "    if fp_saved > 5:\n",
        "        rec = \"Use FNâ‰¤1 (big gain)\"\n",
        "    elif fp_saved > 0:\n",
        "        rec = \"Consider FNâ‰¤1\"\n",
        "    else:\n",
        "        rec = \"Use FN=0 (same)\"\n",
        "    \n",
        "    print(f\"{name:<12} â”‚ {fp_saved:>10} â”‚ {tn_gained:>10} â”‚ {fn_cost:>10} â”‚ {rec:<20}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LEGEND\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "Pool   : Number of actual Large (+) vs Not-Large (-) moves in test set\n",
        "TP     : True Positives (correctly flagged large moves)\n",
        "FP     : False Positives (incorrectly flagged as large - cost: unnecessary hedging)\n",
        "FN     : False Negatives (missed large moves - cost: unhedged exposure!)\n",
        "TN     : True Negatives (correctly rejected non-large events - benefit: no false alarm)\n",
        "FN%    : False Negative Rate = FN / (TP + FN) - want this LOW\n",
        "FP%    : False Positive Rate = FP / (FP + TN) - tradeoff with FN\n",
        "TN%    : True Negative Rate = TN / (FP + TN) = 100% - FP% - shows filtering power\n",
        "Prec   : Precision = TP / (TP + FP) - what fraction of alerts are real\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "> ðŸ“š **Full methodology**: [`docs/cpi_hy_unemployment_methodology.md`](../docs/cpi_hy_unemployment_methodology.md)  \n",
        "> ðŸ”§ **Grid search module**: `src/models/event_grid_search.py`\n",
        "\n",
        "## Exhaustive Grid Search (19,602 configs tested)\n",
        "\n",
        "**Primary constraint**: FN â‰¤ 1% or â‰¤ 5% (catch almost all large moves)  \n",
        "**Secondary**: Maximize TN/FP ratio\n",
        "\n",
        "### Best Configs with FN â‰¤ 1% (100% Recall)\n",
        "\n",
        "| Model | Config | TP | FP | FN | TN | TN/FP |\n",
        "|-------|--------|-----|-----|-----|-----|-------|\n",
        "| **CPIâ†’HY** | LogReg, w=50, 15bp, @0.80 | 8 | 19 | 0 | 77 | **4.05x** |\n",
        "| **Unempâ†’VIX** | RF_shallow, w=50, 2.5pt, @0.20 | 15 | 73 | 0 | 42 | 0.58x |\n",
        "\n",
        "### Best Configs with FN â‰¤ 5% (95%+ Recall)\n",
        "\n",
        "| Model | Config | TP | FP | FN | TN | TN/FP |\n",
        "|-------|--------|-----|-----|-----|-----|-------|\n",
        "| **CPIâ†’HY** | LogReg, w=50, 15bp, @0.80 | 8 | 19 | 0 | 77 | **4.05x** |\n",
        "| **Unempâ†’VIX** | GB, w=20, 2.0pt, @0.05 | 20 | 48 | 1 | 61 | **1.27x** |\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "1. **CPIâ†’HY**: LogReg with high weight (50) is best - AUC=0.911, TN/FP=4.05x\n",
        "2. **Unempâ†’VIX**: Trade-off between FNâ‰¤1% (100% recall, TN/FP=0.58x) vs FNâ‰¤5% (95% recall, TN/FP=1.27x)\n",
        "3. **Higher thresholds** (15bp, 2-2.5pt) enable better TN/FP ratios\n",
        "\n",
        "## TODOs\n",
        "\n",
        "- [ ] Consensus-based surprise (actual vs. analyst)\n",
        "- [ ] Official BLS calendar\n",
        "- [ ] More features (VIX term structure, FedWatch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
